{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc474bf7",
   "metadata": {},
   "source": [
    "# First Assignment - Working with Dependency Graphs  \n",
    "\n",
    "- **Full name**: Matteo Farina  \n",
    "- **Mat. Number**: 221252  \n",
    "- **email**: [matteo.farina-1@studenti.unitn.it](mailto:matteo.farina-1@studenti.unitn.it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3017b6b",
   "metadata": {},
   "source": [
    "### Content  \n",
    "  \n",
    "This notebook contains the code and the explanations concerning the *mandatory* part of the first assignment. Specifically, students were asked to work with [spaCy](https://spacy.io/) and define functions to:   \n",
    "- [Task 1](#Task-1): extract a path of dependency relations from ROOT to a token;  \n",
    "- [Task 2](#Task-2): extract the subtree of dependents given a token;  \n",
    "- [Task 3](#Task-3): check if a given list of tokens (segment of a sentence) forms a subtree;  \n",
    "- [Task 4](#Task-4): identify the head of a span, given its tokens;  \n",
    "- [Task 5](#Task-5): extract sentence subject, direct object and indirect object spans.  \n",
    "  \n",
    "Each task will be presented with a brief explanation of both the goal to achieve and the implemented strategies. Then, each function associated to a task will be presented in the respective code cell. Finally, at the very bottom of each code cell, the core function will be tested and its output will be printed to show you what has actually been achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bdb960",
   "metadata": {},
   "source": [
    "### Requirements  \n",
    "The code in this notebook runs under the **Python v3.9** interpreter. Python dependencies has been managed with *Conda*, but you may use other tools such as *venv* or *virtualenv* as well.\n",
    "  \n",
    "As mentioned in the previous section, **spaCy** is the only library that has to be manually installed to run the code in this notebook:   \n",
    "- To install it, run `pip install spacy`  \n",
    "  \n",
    "Then, also the spacy english model needs to  be installed on your machine. This will be necessary in order to instantiate and perform all the text-processing pipelines across the notebook:  \n",
    "- To install spacy's english model, run `python -m spacy download en_core_web_sm`  \n",
    "  \n",
    "The code cell below instantiates the default example sentences that will be used for testing purposes as well as the text-processing pipeline for the english language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69e2c9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentences: \n",
      "['I saw the man with a telescope.', 'My mom gives me a wonderful gift.', 'We take Natural Language Understanding labs every tuesday.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# load the sentences\n",
    "examples = []\n",
    "with open(\"data/mandatory/examples.txt\", 'r') as f:\n",
    "    [examples.append(line.strip()) for line in f.readlines()]\n",
    "\n",
    "# create the pipeline (will be referenced as a global variable throughout the notebook)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# show sents\n",
    "print(\"Example sentences: \", examples, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238be91d",
   "metadata": {},
   "source": [
    "### Task 1  \n",
    "**Description**: \"extract a path of dependency relations from ROOT to a token\"  \n",
    "  \n",
    "**Strategy**: In order to achieve the goal of the task, which is to define a function that parses a sentence, gets the dependency graph and returns the list of dependency relations from root, I found it useful to split the task into three sub-tasks as follows:  \n",
    "\n",
    "1. **build the list of tokens from ROOT to a given token based on the dependency graph**. This is achieved by the *relpath* function. Starting from a token, it climbs the dependency tree in a bottom-up fashion jumping from head to head. The climbing stops when the root node is met, producing the path from a token to root. Then, this path can simply be reversed to obtain the path from root to the given token.  \n",
    "2. **map each token in the retrieved path to its dependency relation wrt its head**. This is achieved by leveraging spaCy tokens' *dep_* attribute inside the *deprelslist* function and eventually produces the list of dependency relations from root to a given token.\n",
    "3. **do the same for every token in a sentence**. A sentence is parsed using the spacy's default pipeline for english, so tokens are accessible. Then, the actions described in the first two steps are applied to each token and the output (list of dependency relations for each token) is produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2f1d1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I saw the man with a telescope.\n",
      "[['ROOT', 'nsubj'], ['ROOT'], ['ROOT', 'dobj', 'det'], ['ROOT', 'dobj'], ['ROOT', 'prep'], ['ROOT', 'prep', 'pobj', 'det'], ['ROOT', 'prep', 'pobj'], ['ROOT', 'punct']]\n",
      "\n",
      "\n",
      "My mom gives me a wonderful gift.\n",
      "[['ROOT', 'nsubj', 'poss'], ['ROOT', 'nsubj'], ['ROOT'], ['ROOT', 'dative'], ['ROOT', 'dobj', 'det'], ['ROOT', 'dobj', 'amod'], ['ROOT', 'dobj'], ['ROOT', 'punct']]\n",
      "\n",
      "\n",
      "We take Natural Language Understanding labs every tuesday.\n",
      "[['ROOT', 'nsubj'], ['ROOT'], ['ROOT', 'dobj', 'compound', 'compound'], ['ROOT', 'dobj', 'compound'], ['ROOT', 'dobj', 'compound'], ['ROOT', 'dobj'], ['ROOT', 'npadvmod', 'det'], ['ROOT', 'npadvmod'], ['ROOT', 'punct']]\n"
     ]
    }
   ],
   "source": [
    "def relpath(token, mode='bottom-up'):\n",
    "    \"\"\"\n",
    "    Build the backwards path from a token to the root node and returns the list of tokens.\n",
    "    If :param token: is the ROOT, the returned list will contain only :param token: itself.\n",
    "    Params:\n",
    "    - :param token: the token from which to build the path\n",
    "    - :param mode: direction to follow when returning the path ('bottom-up' --> from token to root \n",
    "    or 'top-down' --> from root to token)\n",
    "    \"\"\"     \n",
    "    # check data types\n",
    "    from spacy.tokens import Token\n",
    "    assert type(token) == Token, \"'token' argument of 'relpath' must be of type 'spacy.tokens.Token'\"\n",
    "    assert mode in ('bottom-up', 'top-down'), \"'mode' argument must be one of ('bottom-up', 'top-down')\"\n",
    "    \n",
    "    # return list to be filled\n",
    "    path = [token]\n",
    "    \n",
    "    # build the path\n",
    "    token_ = token\n",
    "    while token_.dep_ != 'ROOT':\n",
    "        head = token_.head\n",
    "        path.append(head)\n",
    "        token_ = head\n",
    "        \n",
    "    # return path from token to ROOT or from ROOT to token according to :param mode:\n",
    "    return path if mode=='bottom-up' else path[::-1] \n",
    "    \n",
    "def deprelslist(token):\n",
    "    \"\"\"\n",
    "    Extract a path of dependency relations from ROOT to :param token:\n",
    "    Params:\n",
    "    - :param token: token up to which to build the dependency list\n",
    "    Returns:\n",
    "    - :deplist: list containing relations from the ROOT node to the TOKEN node.\n",
    "    \"\"\"\n",
    "    # check data types\n",
    "    from spacy.tokens import Token\n",
    "    assert type(token) == Token, \"'token' argument of 'deprels' must be of type 'spacy.tokens.Token'\"\n",
    "    \n",
    "    # grab the branch from root to token\n",
    "    token_list = relpath(token, mode='top-down')\n",
    "    \n",
    "    # map each token to its dependency relation and return\n",
    "    return [t.dep_ for t in token_list]\n",
    "\n",
    "def deprels(sentence):\n",
    "    \"\"\"\n",
    "    Extract the path of dependency relations for each token in :param sentence:.\n",
    "    :return: list of lists. return(i) = list of dependency relations of the i-th token of :param sentence:\n",
    "    \"\"\"\n",
    "    global nlp\n",
    "    doc = nlp(sentence)\n",
    "    return [deprelslist(token) for token in doc]\n",
    "        \n",
    "# test the fn for the complete sentence\n",
    "for sentence in examples:\n",
    "    print(\"\\n\", sentence, deprels(sentence), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82de990f",
   "metadata": {},
   "source": [
    "### Task 2  \n",
    "**Descripton**: \"extract the subtree of dependents given a token\"  \n",
    "  \n",
    "**Strategy**: also for what concerns this task, I thought it could've been useful to 'divide and conquer'. Specifically, the task is addressed with two steps, one that acts on individual units (token-level) and the other that extends it to work on a sentence-level.  \n",
    "1. **retrieve the list of nodes in the subtree of a token**. This is achieved through the *subtree* function, where the spaCy tokens' *subtree* attribute (a generator) is iterated through. The default spaCy's behaviour yields tokens in the exact order they appear in the original sentence. Also, it yields a list with the token itself in case it doesn't have dependents. These features have not been modified, since they were part of the task's specifications.  \n",
    "2. **do the same for each token in a sentence**. To achieve the final goal, a sentence is parsed using the spacy pipeline for the english language. Thus, tokens are accessible so we can iterate through them and get each one's subtree. The output is then returned (list of lists, where internal lists are made of tokens constituting a subtree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40b1989d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I saw the man with a telescope.\n",
      "[[I], [I, saw, the, man, with, a, telescope, .], [the], [the, man], [with, a, telescope], [a], [a, telescope], [.]]\n",
      "\n",
      "\n",
      "My mom gives me a wonderful gift.\n",
      "[[My], [My, mom], [My, mom, gives, me, a, wonderful, gift, .], [me], [a], [wonderful], [a, wonderful, gift], [.]]\n",
      "\n",
      "\n",
      "We take Natural Language Understanding labs every tuesday.\n",
      "[[We], [We, take, Natural, Language, Understanding, labs, every, tuesday, .], [Natural], [Natural, Language], [Understanding], [Natural, Language, Understanding, labs], [every], [every, tuesday], [.]]\n"
     ]
    }
   ],
   "source": [
    "def subtree(token):\n",
    "    \"\"\"\n",
    "    Extracts the subtree of the dependents of :param token: and returns them inside a list.\n",
    "    Tokens are ordered as they appear in the original sentence (spaCy default).\n",
    "    When a token has no dependents, a list containing only the token itself is returned.\n",
    "    \"\"\"\n",
    "    from spacy.tokens import Token\n",
    "    assert type(token) == Token, \"'token' argument of 'subtree' must be of type 'spacy.tokens.Token'\"\n",
    "    return [t for t in token.subtree]\n",
    "\n",
    "def subtrees(sentence):\n",
    "    \"\"\"\n",
    "    Returns every subtree of a sentence inside a list. \n",
    "    Params:  \n",
    "    - :param sentence: input sentence (str)  \n",
    "    \"\"\"\n",
    "    assert type(sentence) == str, \"'sentence' argument of 'subtrees' must be of type 'str'\"\n",
    "    global nlp\n",
    "    doc = nlp(sentence)\n",
    "    return [subtree(token) for token in doc]\n",
    "\n",
    "# test things out\n",
    "for sentence in examples:\n",
    "    print(\"\\n\", sentence, subtrees(sentence), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7c3b62",
   "metadata": {},
   "source": [
    "### Task 3  \n",
    "**Description**: \"check if a given list of tokens (segment of a sentence) forms a subtree\"  \n",
    "  \n",
    "**Strategy**: to check if a list of tokens forms a subtree, a procedure could be checking it against all the possible subtrees of a sentence, as they are represented as lists as well. Therefore, the logic previously implemented in the *subtrees* function comes handy. However, using the exact same function implies generating all possible subtrees before even checking against the first one. For this reason, wanting to allow for early checks and saving computational time as a best practice, a *generator-based subtrees* function has been purposefully implemented.  \n",
    "  \n",
    "To sum up, the **algorithm is so implemented**:  \n",
    ">1. the input sentence is parsed using the spacy pipeline for the english language  \n",
    "2. start iterating through tokens  \n",
    "   2.1. generate token's subtree and compare it to the input list. Stop if a match is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26d22688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is ['with', 'a', 'telescope'] a subtree in sentence 'I saw the man with a telescope.'? True\n",
      "Is ['mom', 'gives', 'me'] a subtree in sentence 'My mom gives me a wonderful gift.'? False\n"
     ]
    }
   ],
   "source": [
    "def subtrees_gen(sentence):\n",
    "    \"\"\"\n",
    "    Exact same behaviour of 'subtrees(sentence)', but yields values instead of returning them.\n",
    "    \"\"\"\n",
    "    assert type(sentence) == str, \"'sentence' argument of 'subtrees' must be of type 'str'\"\n",
    "    global nlp\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        yield subtree(token)\n",
    "\n",
    "def is_subtree(sentence, lot):\n",
    "    \"\"\"\n",
    "    Check if a given list of tokens forms a subtree.\n",
    "    Params:  \n",
    "    - :param sentence: sentence of reference\n",
    "    - :param lot: 'list-of-tokens' to be used to check whether they form a subtree or not\n",
    "    :return: True if :param lot: is a subtree of :param sentence:, False otherwise\n",
    "    \"\"\"\n",
    "    assert type(sentence) == str, \"'sentence' argument of 'is_subtree' must be of type 'str'\"\n",
    "    assert type(lot) == list, \"'lot' argument of 'is_subtree' must be of type 'list'\"\n",
    "\n",
    "    # generate subtrees and compare the ordered list of tokens in the subtree with the one to check against\n",
    "    for stree in subtrees_gen(sentence):\n",
    "        stree_list = [t.text for t in stree]\n",
    "        if stree_list == lot:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# test things out\n",
    "lot_true = ['with', 'a', 'telescope']  # is actually a subtree of the 1st example sentence\n",
    "lot_false = ['mom', 'gives', 'me']  # is not a subtree of the 2nd example sentence\n",
    "print(\"Is {} a subtree in sentence '{}'? {}\".format(lot_true, examples[0], is_subtree(examples[0], lot_true)))\n",
    "print(\"Is {} a subtree in sentence '{}'? {}\".format(lot_false, examples[1], is_subtree(examples[1], lot_false)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e654dc",
   "metadata": {},
   "source": [
    "### Task 4  \n",
    "**Description**: \"identify the head of a span, given its tokens\"  \n",
    "  \n",
    "**Strategy**: this task can be addressed by correctly using the *Span.root* attribute of Spacy's span objects. The correct usage is thereafter enforced with a type check. This way, we can safely get the root of any slice of a sentence.  \n",
    "  \n",
    "**Disclaimer**: because on Piazza it has been written that the input of this function should be \"a sequence of words (not necessarily a sentence)\", the basic implementation below doesn't account for situations where we may want to find the root of a *list of tokens* (where 'list' means python list). Also, I couldn't implement a function that takes as input a common python string, because in that case I would need the parent doc to be passed to the function as well in order to locate the string inside the doc and build the span, but passing also the doc wasn't allowed by the task specifications.  \n",
    "  \n",
    "So, to sum up:\n",
    "- `head(\"with a telescope\")` will raise an `AssertionError` due to type constraints;  \n",
    "- `head([\"with\", \"a\", \"telescope\"])` will also raise and `AssertionError` due to type constraints;  \n",
    "- `head(doc[start_idx:end_idx])` will successfully return the head of the span from `start_idx` (included) to `end_idx` (excluded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "838f61b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of 'with a telescope' is 'with'\n",
      "Head of 'a wonderful gift' is 'gift'\n"
     ]
    }
   ],
   "source": [
    "def head(span):\n",
    "    \"\"\"\n",
    "    Given a span (string representing a contiguous sequence of words in a sentence), return its head.\n",
    "    \"\"\"\n",
    "    from spacy.tokens.span import Span\n",
    "    assert type(span) == Span, \"'span' argument of 'head' must be of type 'spacy.tokens.span.Span'\"\n",
    "    return span.root\n",
    "\n",
    "# test things out\n",
    "docs = [nlp(sent) for sent in examples]\n",
    "span_one = docs[0][4:7]  # 'with a telescope'\n",
    "span_two = docs[1][4:7]  # 'a wonderful gift'\n",
    "print(\"Head of '{}' is '{}'\".format(span_one, head(span_one)))\n",
    "print(\"Head of '{}' is '{}'\".format(span_two, head(span_two)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311c5b71",
   "metadata": {},
   "source": [
    "### Task 5  \n",
    "**Description**: \"extract sentence subject, direct object and indirect object spans\"  \n",
    "  \n",
    "**Strategy**: this task is addressed by leveraging the *dep_* property of spaCy tokens as well as the *subtree* function defined for the second task.  \n",
    "1. A sentence is at first parsed with the spacy pipeline for the english language.  \n",
    "2. Then, tokens and their relations are retrieved in such a way that they are pairwise indexed. This means that `relations[i]` contains a string defining the dependency relation that `tokens[i]` has with respect to its head. In this way, I am able to directly index tokens by their dependency relation and the usage of python's built-in `list.index(elem)` method comes handy.  \n",
    "3. Once that tokens of interests have been retrieved (*nsubj*, *dobj* and *iobj*), their subtree is retrieved too. Obviously, no subtree is retrieved for dependency relations that do not appear in the parsed sentence, so a default empty list is initialized in that case.  \n",
    "4. The algorithm then returns a dictionary with three keys ('nsubj', 'dobj', 'iobj') where the respective values are the spans represented by the retrieved subtrees. So, overall, the output is a dict-of-lists. \n",
    "\n",
    "**Note**: in case of non-projective parses, the subtrees aren't guaranteed to provide a *contiguous* list of tokens. The correct identification of the entities of interest is not affected, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cbfbaa1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nsubj': [I], 'dobj': [the, man], 'iobj': []}\n",
      "{'nsubj': [My, mom], 'dobj': [a, wonderful, gift], 'iobj': [me]}\n",
      "{'nsubj': [We], 'dobj': [Natural, Language, Understanding, labs], 'iobj': []}\n",
      "{'nsubj': [Jet, Blue], 'dobj': [our, flight, which, was, already, late], 'iobj': []}\n"
     ]
    }
   ],
   "source": [
    "def extract(sentence):\n",
    "    \"\"\"\n",
    "    Extract sentence subject, direct object and indirect object spans (if it exists, empty list otherwise.)\n",
    "    :param sentence: input sentence to get the data from\n",
    "    :return: dict containing exactly three keys ('nsubj', 'dobj', 'iobj') with the respective spans\n",
    "    Example:\n",
    "    extract(\"I saw a man.\") --> {'nsubj': ['I'], 'dobj': ['a', 'man'], 'iobj': []}\n",
    "    \"\"\"\n",
    "    assert type(sentence) == str, \"'sentence' argument of 'extract' must be of type 'str'.\"\n",
    "    global nlp\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # get equally indexed tokens and relations\n",
    "    tokens = [token for token in doc]\n",
    "    relations = [token.dep_ for token in tokens]\n",
    "    \n",
    "    # build the span of the nominal subject\n",
    "    nsubj = []\n",
    "    if 'nsubj' in relations:\n",
    "        nsubj = subtree(tokens[relations.index('nsubj')])\n",
    "    \n",
    "    # build the span of the direct object\n",
    "    dobj = []\n",
    "    if 'dobj' in relations:\n",
    "        dobj = subtree(tokens[relations.index('dobj')])\n",
    "        \n",
    "    # build the span of the indirect object (spaCy == 'dative')\n",
    "    iobj = []\n",
    "    if 'dative' in relations:\n",
    "        iobj = subtree(tokens[relations.index('dative')])\n",
    "        \n",
    "    return {'nsubj': nsubj, 'dobj': dobj, 'iobj': iobj}\n",
    "\n",
    "print(extract(examples[0]))  # no iobj\n",
    "print(extract(examples[1]))  # with iobj\n",
    "print(extract(examples[2]))  # more complex sentence\n",
    "print(extract(\"Jet Blue canceled our flight this morning which was already late.\"))  # non projective sent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
